{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/movies.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "keywords = df['keywords'].dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# Для размышлений\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "space = tuple(filter(lambda x: x.lower().startswith('space'), keywords))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "[(1.0, 'space marine'),\n (0.5833333333333334, 'space colony'),\n (0.6086956521739131, 'space opera'),\n (0.5882352941176471, 'space'),\n (0.5714285714285714, 'spaceship'),\n (0.5714285714285714, 'spaceport'),\n (0.7272727272727273, 'space race'),\n (0.72, 'space mission'),\n (0.6666666666666666, 'space pirate'),\n (0.8, 'space carrier'),\n (0.5333333333333333, 'space based weapon'),\n (0.72, 'space station'),\n (0.8, 'spaceman'),\n (0.6153846153846154, 'space invasion'),\n (0.5454545454545454, 'spacecraft'),\n (0.7619047619047619, 'space war'),\n (0.6363636363636364, 'space suit'),\n (0.6666666666666666, 'space travel'),\n (0.6666666666666666, 'space battle'),\n (0.5806451612903226, 'spacecraft accident'),\n (0.6666666666666666, 'space centre'),\n (0.4666666666666667, 'spacecraft officer')]"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(SequenceMatcher(None, space[0], i).ratio(), i) for i in space]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=8000,\n",
    "                      lower=True,\n",
    "                      split=';')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(keywords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "21"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keywords[0].split(';'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def to_binary(words_indexes, num_words=8000):\n",
    "     binary_words = np.zeros(num_words)\n",
    "     binary_words[words_indexes] = 1\n",
    "     return binary_words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([0., 0., 0., ..., 0., 0., 0.])"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(filter(bool, [x[0] if x else 0 for x in tokenizer.texts_to_sequences(keywords[0].split(';'))]))\n",
    "print(len(words))\n",
    "binary_words = to_binary(words)\n",
    "binary_words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "(21, 7979)"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(binary_words == 1).sum(), (binary_words == 0).sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokenizer.pickle', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_4856/2054316723.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtokenizer\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mCustomTokenizer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mmy_tokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mCustomTokenizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\projects\\movie-recommendations\\tokenizer.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, fp, splitter, bag_size)\u001B[0m\n\u001B[0;32m     16\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplitter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msplitter\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbag_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbag_size\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m         all_words = [''] + sorted(\n\u001B[0m\u001B[0;32m     19\u001B[0m             tuple(self.tokenizer.index_word), key=lambda x: x[0])\n\u001B[0;32m     20\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mall_words\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mall_words\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mall_words\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m169\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\projects\\movie-recommendations\\tokenizer.py\u001B[0m in \u001B[0;36m<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m     17\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbag_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbag_size\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m         all_words = [''] + sorted(\n\u001B[1;32m---> 19\u001B[1;33m             tuple(self.tokenizer.index_word), key=lambda x: x[0])\n\u001B[0m\u001B[0;32m     20\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mall_words\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mall_words\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mall_words\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m169\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mall_words\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mempty\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbag_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from tokenizer import CustomTokenizer\n",
    "my_tokenizer = CustomTokenizer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_8184/3455338021.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mmy_tokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex_word\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'my_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "my_tokenizer.tokenizer.index_word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}